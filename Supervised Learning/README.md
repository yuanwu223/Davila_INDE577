# Supervised Learning Projects

This repository contains hands-on implementations of various **Supervised Learning** algorithms as part of the **Davila_INDE577** course. Each folder includes relevant notebooks, datasets (if applicable), and visualizations to help understand how these algorithms work in practice.

## üìö Table of Contents

- [Decision Trees](#decision-trees)
- [K-Nearest Neighbors (KNN)](#k-nearest-neighbors-knn)
- [Linear Regression](#linear-regression)
- [Logistic Regression](#logistic-regression)
- [Neural Networks](#neural-networks)
- [Random Forests](#random-forests)
- [The Perceptron](#the-perceptron)
- [XGBoost](#xgboost)

---

## Decision Trees

This folder contains an implementation of Decision Tree classification using `scikit-learn`, including training, visualization (`DecisionTrees.png`), and evaluation metrics.

‚û°Ô∏è [Go to folder](./DecisionTrees)

---

## K-Nearest Neighbors (KNN)

This project explores the KNN algorithm for classification. The dataset `heart.csv` was previously included for a heart disease prediction example.

‚û°Ô∏è [Go to folder](./KNearestNeighbors)

---

## Linear Regression

A classic regression approach implemented with visual plots and evaluation metrics. Includes an image visualization for the regression fit.

‚û°Ô∏è [Go to folder](./LinearRegression)

---

## Logistic Regression

Binary classification using logistic regression, implemented with clear explanation and plots to demonstrate model performance.

‚û°Ô∏è [Go to folder](./LogisticRegression)

---

## Neural Networks

Implements a basic feedforward neural network using libraries such as `Keras` or `Tensorflow` . The folder contains all relevant training scripts and visual outputs.

‚û°Ô∏è [Go to folder](./NeuralNetworks)

---

## Random Forests

A powerful ensemble method based on Decision Trees. This project includes training, evaluation, and feature importance analysis.

‚û°Ô∏è [Go to folder](./RandomForests)

---

## The Perceptron

An implementation of the original Perceptron algorithm for binary classification, showing how weight updates work and how convergence is achieved.

‚û°Ô∏è [Go to folder](./ThePerceptron)

---

## XGBoost

This folder demonstrates the use of the gradient boosting framework **XGBoost**. Includes hyperparameter tuning and performance evaluation on structured data.

‚û°Ô∏è [Go to folder](./XGBoost)

---

## üìå Notes

- All code is written in Jupyter Notebooks for ease of understanding.
- The focus is on both clarity and practical application.
- Visualizations are used extensively to aid in comprehension.

---

## üí° License

This project is open source under the [MIT License](LICENSE).
